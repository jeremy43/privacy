\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{caption}
\usepackage{array}
\usepackage{subfig}
\usepackage{bbm}
\usepackage{color}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{algorithmic,algorithm}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
	\textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\mb}[1]{\boldsymbol{#1}}

\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\Cov{\mathrm{Cov}}
\def\Var{\mathrm{Var}}
\def\half{\frac{1}{2}}
\def\th{\mathrm{th}}
\def\tr{\mathrm{tr}}
\def\df{\mathrm{df}}
\def\dim{\mathrm{dim}}
\def\col{\mathrm{col}}
\def\row{\mathrm{row}}
\def\nul{\mathrm{null}}
\def\rank{\mathrm{rank}}
\def\nuli{\mathrm{nullity}}
\def\sign{\mathrm{sign}}
\def\supp{\mathrm{supp}}
\def\diag{\mathrm{diag}}
\def\Gauss{\mathrm{Gauss}}
\def\subsample{\mathsf{PoissonSample}}
\def\sparse{\mathsf{SparseVector}}
\def\Lap{\mathsf{Lap}}
\def\rr{\mathsf{Random Response}}
\def\lap{\mathsf{Laplace Mechanism}}
\def\expon{\mathsf{Exponential Mechanism}}
\def\aff{\mathrm{aff}}
\def\hy{\hat{y}}
\def\ty{\tilde{y}}
\def\hbeta{\hat{\beta}}
\def\tbeta{\tilde{\beta}}
\def\htheta{\hat{\theta}}
\def\halpha{\hat{\alpha}}
\def\hf{\hat{f}}
\def\lone{1}
\def\ltwo{2}
\def\linf{\infty}
\def\lzero{0}
\def\T{^T}
\def\R{\mathbb{R}}
\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cD{\mathcal{D}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cH{\mathcal{H}}
\def\cI{\mathcal{I}}
\def\cJ{\mathcal{J}}
\def\cO{\mathcal{O}}
\def\cL{\mathcal{L}}
\def\cM{\mathcal{M}}
\def\cN{\mathcal{N}}
\def\cP{\mathcal{P}}
\def\cQ{\mathcal{Q}}
\def\cR{\mathcal{R}}
\def\cS{\mathcal{S}}
\def\cT{\mathcal{T}}
\def\cW{\mathcal{W}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}
\def\TV{\mathrm{TV}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem*{remark}{Remark}
\newtheorem{remark-star}{Remark}
\newtheorem{remark-star-1}{Remark}
\newtheorem*{observe}{Observation}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{propsition}{Proposition}
\newtheorem*{proof-sketch}{Proof Sketch}

\title{Differential Private Knowledge Transfer  under Covariate and Label Shift}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
	Yuqing Zhu \thanks{Equal contribution.}\\
	Department of Computer Science \\
	University of California, Santa Barbara \\
	Santa Barbara, CA 93106 \\
	\texttt{yuqingzhu@cs.ucsb.edu} \\
	% examples of more authors
	\And
	Chong Liu \printfnsymbol{1} \\
	Department of Computer Science \\
	University of California, Santa Barbara \\
	Santa Barbara, CA 93106 \\
	\texttt{chongliu@cs.ucsb.edu} \\
	\And
	Yu-Xiang Wang \\
	Department of Computer Science \\
	University of California, Santa Barbara \\
	Santa Barbara, CA 93106 \\
	\texttt{yuxiangw@cs.ucsb.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
	% Context, Gap, How we close the Gap, Results.
    We
\end{abstract}

\section{Introduction}
% Big data, esp., Big labeled data, fueled the AI Revolution.

% As of today, we are able to reach and surpass human performance in cognitive tasks such as recognizing wildlife in images, performing conversational speech recognition and translating Chinese text into English and 

% As data size and computation skyrocket, getting label becomes the a very costly process.
% talk about expert labels, then talk about crowdsourcing.

% To do this labeling task, one may ask a lot of non-professional people to label them. This kind of labeling framework is crowdsourcing where these non-professional people are usually called workers. By crowdsourcing, lots of labels can be obtained with low cost while it also leads to some problems.


% talk about how this motivates crowdsourcing models/algorithms. Give example of crowdsourcing labeled data sets. Give example of when it hits the KDD community, quote Sheng, Provost, Ipeirotis.


\subsection{Related Work}
% pate
\cite{papernot-iclr18-scalable}
\cite{bassily-nips18-model}
\cite{papernot-iclr17-semi}

\cite{dwork-14-the}

% covariate shift and label shift, related to casual inference
\cite{lipton-icml18-detecting}
\cite{bickel-jmlr09-discriminative}
\cite{mansour-colt09-domain}
\cite{cortes-nips10-learning}
Also, covariate shift and label shift can be understood as cause-effect learning and effect-cause learning in casual inference community \cite{scholkopf-icml12-on}.
\cite{zhang-icml13-domain}
\cite{gretton-chapter-covariate}
\cite{azizzadenesheli-iclr18-regularized}

\cite{Ji-ml13-differential}
\cite{wang-ecml18-differentially}
\cite{chen-aistat19-renyi}

\subsection{Our Contributions and Results}
% talk about our contributions
% 

\section{Problem setup}
\subsection{Symbols and Notations}
Suppose now we have two different distributions $\mathcal{P}$ and $\mathcal{Q}$, and we have the items and their labels $(x_i,y_i)$ drawn i.i.d. from $\mathcal{P}$ where $i\in [n]$ is the index of the item and $n$ is the number of items. Also, we have $(\hat{x_j},\hat{y_j})$ drawn i.i.d. from $\mathcal{Q}$ where $j\in [m]$ is the index of the items and $m$ is the number of items.

\subsection{Problem Statement}
\textbf{Domain Adaptation}
Let $\mathcal{F}$ denote the function class, the goal of domain adaptation is to find a function $f\in \mathcal{F}$ such that
\begin{align*}
\inf_{f\in \mathcal{F}} \mathbbm{E}_{\mathcal{Q}}[ \ell_f (x,y)]
\end{align*}

\textbf{PATE}
In PATE, the data $(x_i, y_i)$ from distribution $\mathcal{P}$ is private, and the data $(\hat{x_j}, \hat{y_j})$ is public. The goal of PATE is to find a function $f$ using $(\hat{x_j}, \hat{y_j})$ and $(\epsilon,\delta)-DP$ using $(x_i, y_i)$ such that
\[
\inf_{f\in \mathcal{F}} \cE_{\mathcal{P}}[ \ell_f (x,y)]
\]

\section{Proposed Approach}


\section{Under label shift}
The label shift assumption is
\[
P_{\mathcal{P}}(x|y) = P_{\mathcal{Q}}(x|y)
\]

We train $k$ classifiers $f$ in the training dataset, and we want to preserve privacy for samples in training. And $f$ should approximate the probability of the labels given the items
\[
f(x)\approx P_\mathcal{P}(y|x)
\]

We use the classifiers trained on the distribution $\mathcal{P}$ and public data whose distribution is $\mathcal{Q}$, thus, the joint distribution is
\[
(x,\hat{f}(x))\sim P_\mathcal{Q}(x)P_\mathcal{P}(y|x)
\]

The expected loss on the distribution $\mathcal{P}$ is
\begin{align*}
\mathbbm{E}_{\mathcal{P}}[\ell_f(x,y)]&=\int_{(x,y)}P(x,y)\ell_f(x,y){\rm d}x{\rm d}y\\
&\approx \frac{1}{m}\sum_{i=1}^m \frac{P_\mathcal{P}(\hat{x})}{P_\mathcal{Q}(\hat{x})}\ell(\hat{x}_i,f(\hat{x}_i))
\end{align*}
The expected loss that we can approximate is
\begin{align*}
\mathbbm{E}_{\hat{x}\sim \mathcal{Q},\hat{y}\sim P(y|\hat{x})}[\ell_f(\hat{x},\hat{y})]&=\int_{(\hat{x},f(\hat{x}))}P_\mathcal{Q}(\hat{x})P_\mathcal{P}(y|\hat{x})\ell(\hat{x},y){\rm d}x{\rm d}y\\
&\approx \frac{1}{m}\sum_{i=1}^{m}\ell(\hat{x}_i,f(\hat{x}_i))
\end{align*}
Thus, the key problem becomes how to estimate
\[
\frac{P_\mathcal{P}(\hat{x})}{P_\mathcal{Q}(\hat{x})}
\]
which is $\hat{w}^{-1}=\hat{c}_{\hat{y},y}\hat{\mu}^{-1}_{\hat{y}}$ from the label shift. We can add privacy noise to $\hat{c}_{\hat{y},y}$. When query $x_i$, we compute $f(x)\cdot w$.

\section{Covariate shift correction}
For covariate shift, we make the simplifying assumption that $P_{\cP}(x,y)$ and $P_{\cQ}{x,y}$ only differ via $P_{\cP}(x,y) = P(y|x)P_{\cP}(x)$ and $P_{\cQ}(x,y) = P(y|x)P_{\cQ}(x)$. In other words, the conditional probabilities of $y|x$ remain unchanged. For importance sampling, we begin by minimizing the expected risk of Private dataset $\cP$.
\begin{align*}
\mathbbm{E}_{\mathcal{P}}[\ell_f(x,y)]&=\int_{(x,y)}P(x,y)\ell_f(x,y){\rm d}x{\rm d}y\\
&\approx \frac{1}{m}\sum_{i=1}^m \frac{P_\mathcal{P}(\hat{x})}{P_\mathcal{Q}(\hat{x})}\ell(\hat{x}_i,f(\hat{x}_i))
\end{align*}
We apply logistic regression to estimate importance weight $\frac{P_\mathcal{P}(\hat{x})}{P_\mathcal{Q}(\hat{x})}$. Suppose there are $K$ classes in both private and public dataset, for each class $k$, we want to use a logistic regression approach to estimate $\frac{p_k(z=1|x)}{z=-1|x}$. Here $z_i$ is a 1 if data drawn from private dataset and $-1$ for data drawn from $Q$ public dataset. Then the probability in a mixed dataset is given by
$$
p(z=1|x) =\frac{p_{\cP(x)}}{p_\cP(x)+p_\cQ(x)}
$$
We define $p(z=1|x) = \frac{1}{1+\exp{-f(x)}}$, where $f(x) = x^T \cdot theta$, $\theta$ is parameter in logistic regression.  So $\frac{p(z=1|x)}{z=-1|x} = e^{f(x)}$.Now we need to solve the differentially private logistic regression problem to obtain $f$.
\begin{enumerate}
	\item Train multi-classifiers on private dataset, and privately label public dataset with privacy budget$(\epsilon_1, \delta_1)$.
	\item Train binary logistic regression to obtain importance weight parameter $f(x)$.
	with privacy budget$(\epsilon_2, \delta_2),\minimize_f \frac{1}{2n} \sum_{(x,z)} \log{1 + \exp(-zf(x))} +\frac{1}{2\lambda}||f||^2 $\red{do we need to train k classifers?}. 
	\item Since the size of private dataset is usually overwhelming than public dataset, here we also implement an importance weighting version of logistic regression.
	\begin{align*}
	  \beta = \argmin_\beta -\frac{1}{N_p} \sum_{x\in \text{Private}} \log( p( x\in \text{Private} | x \in \text{Private and Public}))\\
  - \frac{1}{N_s}\sum_{x \in \text{Public}} \log(p(x \in \text{Public} |\text{Public and Private}))
	\end{align*}
	\item use $e^{f(x_i)}$ as covariate shift correction weights in training public dataset.
\end{enumerate}

\section{Theoretical Results}
In this section, we provide several theoretical results of our proposed algorithms, including privacy guarantees and utility guarantees.

\subsection{Privacy Guarantees}
\begin{theorem}[Error bounds]
	Assume $\hat{w} = w + E_0$, we have 
	$$
	||	\diag(\hat{w})^{-1} -  \diag(w)^{-1}|| \leq 0.5|| \diag(w)^{-1}||^2 ||E_0||
	$$
\end{theorem}
\begin{proof}
	Assume $\hat w  = \hat C_{\hat y, y}^{-1}\hat{\mu}_{\hat y} $, $w = C_{\hat y, y}^{-1} \mu_{y}$, we already have $||w - \hat w|| \leq E_0$. Here $w_i = \frac{q(y_i)}{p(y_i)}$. In Pate framework, we are interested in $ w^{-1} = \frac{p(y)}{q(y)}$, (the element-wise inverse of $w$).
	\begin{align}
	|| \hat w^{-1} -w^{-1}||^2 = || {\diag (\hat w)}^{-1} - {\diag( w)}^{-1}||^2
	\end{align}
	By Woodbury matrix identity, we have 
	\begin{align}
	\diag(\hat{w})^{-1} &= [\diag(C_{\hat y, y}^{-1}\mu_{y} + E_0)]^{-1} \\
	&= \diag(w)^{-1} + \diag(w)^{-1}[\diag(w)^{-1} + \diag(E_0)^{-1}]^{-1}\diag(w)^{-1}
	\end{align}
	$||[\diag(w)^{-1} + \diag(E_0)^{-1}]^{-1}|| \leq||0.5E_0|| =0.5||E_0||$\\
	So we have $||	\diag(\hat{w})^{-1} -  \diag(w)^{-1}|| \leq 0.5|| \diag(w)^{-1}||^2 ||E_0||$
\end{proof}
In order to privately release importance weight $w^{-1}$, we add gaussian noise to $\hat{C}_{\hat y, y}$, and we use the following lemma to estimate the empirical risk and generalization error under label shift setting.
\begin{lemma}[privacy guarantee for releasing $\hat C_{\hat y, y}$.]
	Suppose $D,D'$ are adjacent dataset.The global sensitivity $\triangle$ for each entry in the $C_{y_i, y_j}$ is at most $\frac{1}{n}$. We set $\sigma = \frac{\sqrt{2 \log(1.25/\delta)}}{n \epsilon}$,add gaussian noise $E_{\cN} $ joint from $\cN(0, \sigma^2)$ to
	$\hat C_{\hat y, y}$.  This method will ensure a $(\epsilon, \delta)$ guarantee for releasing $\hat C$.
\end{lemma}\label{privacy_label}
\begin{lemma}[{utility guarantee for $||{\diag(\tilde{w})}^{-1} - {\diag(w)}^{-1}||$} under label shift] 
	
	if we privately release $\hat{C}_{\hat y,y}$ with gaussian noise $\cN$, and assume $\hat{C}_{\hat y,y} = C_{\hat y, y} + E_1$, then $||{\diag(\tilde{w})}^{-1} - {\diag(w)}^{-1}|| \leq \frac{(||E_1||^2 + ||\cN||^2)}{\sigma_{\min}^2}||w||^2 + ||E_3||$.
\end{lemma} 
\begin{proof}
	Note that $\hat C_{\hat y, y} = \frac{1}{n}\sum_{i=1}^nf(x_i)f(y_i)^T$, where $e_y$ is the standard basis with 1 at the index of $y \in \cY$ and 0 elsewhere. $E e_{f(x_i)}e_{y_i}^T = C_{\hat y,y}$. From \eqref{privacy_label}, we have differential private $\tilde {C}_{\hat y, y} = \hat C_{\hat y, y}  + \cN$, and we want to bound $||\tilde C_{\hat y,y} - C_{\hat y, y}||$.
	\begin{align}
	||\tilde C_{\hat y,y} - C_{\hat y, y}|| & \leq ||\tilde C_{\hat y,y} -\hat C_{\hat y, y}|| + ||\hat C_{\hat y,y} - C_{\hat y, y}|| \\
	& = ||\cN|| + ||E_1||
	\end{align}
	Denote $Z_i = e_{f(x_i)}e_{y_i}^T - C_{\hat y , y}$, $||Z_i|| \leq 2$, by matrix Berstein inequality we have for all $t \geq 0$:
	$$
	\mathbb{P}(||E_1|| \geq t \diagup n) \leq 2ke^{\frac{-t^2}{n+2t/3}}
	$$
	Take $t = \sqrt{20nlogn}$, then with probability at least $1 - 2kn^{-10}$, $||E_1|| \leq \sqrt{\frac{20 \log n}{n}}$
	For $||\cN||$, suppose there are $d$ classes, $||\cN|| \leq \sigma \sqrt{d} = \frac{\sqrt{2 \log(1.25 /\delta)}}{n \epsilon}$. Note $\tilde w = \tilde C_{\hat y, y} \mu_{\hat y}$, we get 
	$$
	||\tilde w  -w||^2 \leq \frac{(||E_1||^2 + ||\cN||^2)}{\sigma_{\min}^2}||w||^2 + ||E_3||
	$$
	$||E_3|| =||\mu_{\hat y} -\hat{\mu}_{\hat y}||_2 \leq{\frac{\sqrt{10k log m}}{\sqrt(m)}}$
\end{proof}
\begin{remark}[privacy guarantee for $\beta$ under covariate shift.] The following algorithm will provide $(\epsilon, \delta)$ privacy guarantee for $\beta$.
	Assume dataset $\cD = P \bigcup Q =\{x_1 ,....,x_{n+m}\}$, convex loss function $\hat{\cL}(\beta; \cD) = \frac{1}{n}\sum_{i=1}^n \ell(\beta; x_i)$, $\ell$ here is binary logistic regression with continuous Hessian, $||\nabla \ell(\beta;x)|| \leq \xi$ and the smooth term $\lambda$ of $\nabla^2 \ell(\beta; x)$. \\
	Set $\triangle \geq \frac{2\lambda}{\epsilon}$, sample $b \in \mathbb{R}^k$ from $\cN(0, \frac{\xi^2(8 log(\frac{2}{\delta}+4\epsilon))}{\epsilon^2}I_{k \times k})$. \\Return $\beta^{priv} = \argmin_{\beta \in \mathbb{F}} \hat{\cL}(\beta; \cD) + \frac{1}{(n+m}r(\beta) +\frac{\triangle}{2(n+m)}||\beta||_2^2+ \frac{b^T\beta}{n+m}$ . 
\end{remark}
\begin{lemma}[utility guarantee for $\beta$ under covariate shift]. let $\hat J(\theta; P,Q) = \frac{1}{n}\sum_{i=1}^n \ell(\beta ; x_i) + \frac{r(\beta)}{n}]$. Let $\beta = \argmin_{\beta \in \mathbb{F}} \hat J(\beta;P,Q) + \frac{\triangle}{2n}||\beta||_2^2$ and let $\beta^{priv}$ be the output of Obj-Pert, where $\mathbb{F}$ is a closed convex set. Then
	$$
	||\beta - \beta^{priv}||_2 \leq \frac{2||b||_2}{\triangle}
	$$
	Proof from Appendix D.(Kifer)
\end{lemma}

\subsection{Utility Guarantees}
We provide utility guarantees under both realizable and unrealizable cases. Let $R_P$ and $R_Q$ denote the risk of a hypothesis $h\in \cH$ on distribution $P$ and $Q$, respectively, namely,
\begin{align*}
R_P &= \min_{h\in \cH} \text{err}(h,P),\\
R_Q &= \min_{h\in \cH} \text{err}(h,Q).
\end{align*}
Totally, we need to consider four different cases: (1) $R_P=0, R_Q=0$, (2) $R_P>0, R_Q>0$, (3) $R_P=0, R_Q>0$, and (4) $R_P>0, R_Q=0$. The first case is the realizable case, while other three are unrealizable ones. The following lemma shows that the last case never happens and it could be ignored.
\begin{lemma}
	Suppose if $P(x,y) > 0$, then $Q(x,y)>0$. If $\cH$ is realizable for $Q$, then $\forall x,y, \exists h\in \cH$, such that $Q(x,y)>0$ and $h(x)=y$. Therefore, $\cH$ is also realizable on $P$.
\end{lemma}
\begin{proof}
	By the assumption, $\forall x,y$ such that $P(x,y)>0$, we can get $Q(x,y)>0$. Considering the error on $Q$, we could write
	\begin{align*}
	\text{err}(h^*_Q,Q)=\sum_{x,y} Q(x,y)\mathbbm{1}(h^*_Q(x)\neq y)=0,
	\end{align*}
	where $h^*_Q$ is the realizable hypothesis and $\mathbbm{1}(\cdot)$ is the indicator function. So for error on $P$, we could write
	\begin{align*}
	\text{err}(h^*_Q,P)=\sum_{x,y} P(x,y)\mathbbm{1}(h^*_Q(x)\neq y)=0,
	\end{align*}
	which means $\cH$ is also realizable on $P$.
\end{proof}
\begin{remark}
	Throughout this paper, we are considering the importance weight $w=P(x,y)/Q(x,y)$. The assumption that if $P(x,y) > 0$, then $Q(x,y)>0$ is not artificial. It means $w<\infty$, i.e., $w$ is bounded.
\end{remark}

\section{Experiments}
\subsection{Experimental Settings}
\begin{enumerate}
	\item we test our model on Mnist and Adult dataset. For each dataset, we leave the training dataset as the private dataset, and simulate label shift or covariate shift on test dataset.
	\item  For covariate shift, we generate bias sampling schemes over the features following \cite{gre_cov}. Specifically,  we first did PCA on public dataset, and select the first principal component of the training data and the corresponding projection values. Denoting the minimum value of the projection as $m$ and the mean as $\bar{m}$, we applied a normal distribution with mean $m + (\bar{m}-m)/a$ and variance $(\bar{m}-m)/b$ as the biased sampling scheme. A large $a$ will simulate a more apparant covariate shift, here we compare the experiements under different covariate shift setting.
	\item To solve a differential private logistic regression problem, we adopt the object perturbation by Kifer. 
	\item For label shift simulation, we adopt two sampling schemes. In $\textbf{knock-out shift}$, we knock out a fraction $\delta$ of data points from a given class from test and validation sets.(We assume the training set is the private dataset). In \textbf{Dirichlet shift}, we draw $p_\cQ(y)$ from a DIrichlet distribution with concentration parameter $\alpha$. With uniform $p_{\cQ}(y)$, Dirichlet shift is bigger for smaller $\alpha$.
\end{enumerate}

\subsection{Experimental Results}
\begin{table}[]
\begin{tabular}{lllllll}
knock out              & weight\_teacher & weight\_student & unweight\_teacher & un\_weight\_student & dataset & details                                                      \\
 0.01 & 0.91            & 0.905           & 0.895             & 0.935               & mnist   & 50 teacher, lr = 0.01, weight divided by sum then time to 10 \\
                    &                 &                 &                   &                     &         &                                                              \\
                    &                 &                 &                   &                     &         &                                                              \\
                    &                 &                 &                   &                     &         &                                                             
\end{tabular}
\end{table}
\begin{table}[]
\begin{tabular}{lllllll}
Dirchlet           & weight\_teacher & weight\_student & unweight\_teacher & un\_weight\_student & dataset & details                                                      \\
 0.1 & 0.76           & 0.84           & 0.71          & 0.9`             & mnist   & 50 teacher, lr = 0.01, weight divided by sum then time to 10 \\
                    &                 &                 &                   &                     &         &                                                              \\
                    &                 &                 &                   &                     &         &                                                              \\
                    &                 &                 &                   &                     &         &                                                             
\end{tabular}
\end{table}



\section{privacy guarantee and utility guarantee}
This section follows from model agnostic methods.
\begin{algorithm}[htb] 
\caption{ $\cA_{\text{SubSample}}$Online Query Release via sub-sample and aggregate}
\label{alg:Framwork} 
\begin{algorithmic}[1] 
\REQUIRE ~~\\ 
dataset $D$, query Set $\cF = \{f_1, ..., f_m\}$ chosen online, range of the queries $\{ \cR_1, ..., \cR_m\}$, unstable query cutoff: $T$, privacy parameter $\epsilon, \delta > 0$, failure probability $\beta$\\
The set of unlabelled samples for current batch, $U_n$;\\
$k \gets 136\cdot \log(4mT  /\min(\delta, \beta/2)) \sqrt{(T\log(2/\delta))}/\epsilon$;\\
Arbitrarily split $D$ into $k$ non-overlapping chunks of size $n/k$, Call them $D_1, ..., D_k$;\\
\ENSURE ~~\\
\FOR{ $i$ \ $\in [m]$ }
\STATE Let $\cS_i = \{ f_i(D_1), ..., f_i(D_k)\} $, and for every $r \in \cR$, let $ct(r) = \text{times r appears in } \cS_i$;\\
\STATE $ \hat{f_i(D)} \gets \argmax_{r \in \cR_i}$, $dist_{\hat{f_i}} \gets \max \{ 0, (\max_{r \in \cR_i}[ct(r)] - max_{r \in \cR_i \setminus \hat{f_i(D)}}-1 \}$
\ENDFOR
\STATE Output $\cA_{OQR}(D, \{\hat{f_1}, ..., \hat{f_m}\}, \{dist_{\hat{f_1}}, ..., \{dist_{\hat{f_m}}\}, T, \epsilon, \delta$; 
\label{algorithm1}
\end{algorithmic}
\end{algorithm}

\begin{theorem}(utility guarantee)
Let $\cF$ denote any set of $m$ adaptively chosen queries, and $D$ be a dataset of $n$ samples drawn i.i.d from a fixed distribution $\cD$. For $ k = 136\cdot \log(4mT  /\min(\delta, \beta/2)) \sqrt{(T\log(2/\delta))}/\epsilon$ , let  $\bar{L} \subseteq  \cF$ be a set of queries s.t for every $f \in \bar{L}$, there exists some $x_f$ 
for which $f(\hat(D)=x+f$ w.p. at least 3/4 over drawing a dataset $\hat(D)$ of $n/k$ i.i.d. data samples from $\cD$.  If $|\tilde{L}| \geq m-T$, then w.p. at least $1 -\beta$ over the randomness of algorithm $\cA_{\text{subSamp}}$
\end{theorem}

\begin{example}
Take Mnist dataset as an example, where the size of private dataset is 60000(teacher), the size of public dataset is.10000(student). So $m=10000$, we need to answer m queries. Suppose $\delta = 1e-5, \epsilon = 1, \beta >> \delta, T = 100$, then $ k > 10^5/$, which is not achievable.
\end{example}
So agnostic model provides a way to derive utility guarantee for pate framework, such as with certain probability, the teacher is able to assign true label for each students.  If we want to combine it with cov-shift and label shift, rewrite differently private importance weight into a utility guarantee version, and then do a composition.
\section{Conclusion}

\bibliography{pate_shift.bib}
\bibliographystyle{plain}

\end{document}